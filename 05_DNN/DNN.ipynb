{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CX_9BuPB_hA-"
   },
   "source": [
    "# Day 5: Introduction to Deep Neural Networks!\n",
    "\n",
    "In this tutorial, we're going to go over the fundimentals of Neural Networks, specifically focusing on **Dense Neural Networks**. (If you're interested in Convolutional Neural Networks, don't fret! You'll probably enjoy the next lecture ;) ) \n",
    "\n",
    "Neural Networks as a whole are an _incredibly_ broad and complex topic. As we only have half a day today, we're not going to be able to cover much more than the basic ideas and techniques, but the hope is that this provides a stable base for you to continue building your knowledge on top of. If you don't itend to continue much farther down the machine learning rabbit hole, we hope that at the very least, this can serve to de-mystify neural networks/machine learning as a whole :) \n",
    "\n",
    "**Learning Objectives:**\n",
    "* Understand Vectors, Matricies, and Dot Product\n",
    "* Understand how a single layer perceptron works, along with batching and gradient descent.\n",
    "* Learn how to build and train a simple Deep Neural Network (Multi-Layer Perceptron - MLP) using the TensorFlow/Keras deep learning python framework\n",
    "* Learn about different optimizers beyond (Stochastic) Gradient Descent and touch on when you might want to use them\n",
    "* Touch on a few key \"Hyperparameters\" that you, the human, can and will need to optimize when designing a neural network (Learning Rate, Batch Size, etc.) \n",
    "\n",
    "\n",
    "**Other Resources**\n",
    "* a\n",
    "* b\n",
    "* c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Me Ahead of time!\n",
    "from sklearn.datasets import fetch_openml\n",
    "jet_tagging_data = fetch_openml('hls4ml_lhc_jets_hlf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Notation, Terms, and other background info\n",
    "In this notebook, you'll likely see some new symbols, terminology, and other topics that you haven't come across before. Don't be afraid!\n",
    "\n",
    "Most of the math symbols you see here simply function as *shorthand* for different math concepts, so that we can write an equation using these concepts without needing to define \"generic\" variables or functions every time we reference them. We've compiled a list of these terms here for you to reference when needed, but at least skimming through these before continuing is reccomended. \n",
    "\n",
    "Additionally, there are some math concepts that are useful to know about/have a basic understanding of, as they're used quite heavily in this notebook (and Neural Networks/Machine Learning in general!)\n",
    "\n",
    "We're going to assume that you have a baseline level of knowledge regarding some math concepts, but if you run into something that you don't understand (regardless of if it's defined here or not!), please let us know and we'll be happy to explain it! Additionally, some definitions of terms might include Machine Learning concepts that are covered later in the notebook (e.g. *Activation Functions*), we'll include another list of terms/symbols that are covered as part of this notebook at the end, so for now don't worry too much if you run into one of these terms. \n",
    "\n",
    "## Math Concepts\n",
    "\n",
    "###  Vectors & Scalars \n",
    "a **vector** is a kind of variable that not only has a value (aka *magnitude*), but also a *direction*. One common way to represent and define vectors that we'll use is by stating the change along each axis (the *components*) the vector travels in (in 2D, this would be $x$ and $y$ *components*, in 3D, the $x$, $y$ and $z$ *components* ). \n",
    "\n",
    "This can be written as $\\vec{v} = (x,y)$ _or_ as $\\boldsymbol{v} = (x,y)$ (note the second $a$ is bolded). Sometimes, the *components* are written stacked vertically such as $\\vec{v} = \\begin{pmatrix} x\\\\ y\\\\ \\end{pmatrix}$\n",
    "\n",
    "In this format, calculating the *magnitude* of a vector, written as $| \\vec{v} |$, is done by applying the pythagorean theorm!\n",
    "\n",
    "<img src=\"img/vector_components.png\" alt=\"Plot labeling the components of a vector, plus the definition for mangitude\" style=\"background-color:white; width:400px;\" />\n",
    "\n",
    "* You can add or subtract vectors to/from vectors, this will produce a vector as a result ( the *resultant*) \n",
    "* Multiplication of a Vector with a Scalar produces a vector\n",
    "* Multiplication of Vectors with Vectors can produce either:\n",
    "    * Another Vector, by calculating the *Cross Product*: $\\vec{v} \\times \\vec{b} = \\vec{vb}$\n",
    "    * a *Scalar*, by calculating the *Dot Product*: $\\vec{v} \\cdot \\vec{b} = vb$ \n",
    "* You cannot divide with vectors\n",
    "    \n",
    "a **scalar** is a normal value, without a direction. Scalars are considered to only have a *magnitude*. Most math you know and have done throughout school has been **scalar** math :)\n",
    "\n",
    "\n",
    "### Matrix/Matrices \n",
    "a **Matrix** is a _structured_ table/an array of numbers. Location of each element matters, and they can be **1 or more dimensions** in shape. Matricies are generally labeled as capital/uppercase letters, and are represented with it's elements placed in a rectangular grid, encapsulated by large square brackets $[  ]$ or parenthesis $(  )$  A Matrix with $m$ rows and $n$ columns can be written as    \n",
    "$$ \n",
    "A = [a_{m,n}] = \\begin{bmatrix} \n",
    "  {a}_{1,1} & \\dots & {a}_{1,n}\\\\ \n",
    "  \\vdots & \\ddots & \\vdots\\\\ \n",
    "  {a}_{m,1} & \\dots & {a}_{m,n}\\\\ \n",
    "\\end{bmatrix} = \\begin{pmatrix} \n",
    "  {a}_{1,1} & \\dots & {a}_{1,n}\\\\ \n",
    "  \\vdots & \\ddots & \\vdots\\\\ \n",
    "  {a}_{m,1} & \\dots & {a}_{m,n}\\\\ \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "So if we set $m=4$ and $n=2$\n",
    "$$\n",
    "A = [a_{4,2}] = \\begin{bmatrix} \n",
    "  {a}_{1,1} & {a}_{1,2}\\\\ \n",
    "  {a}_{2,1} & {a}_{2,2}\\\\ \n",
    "  {a}_{3,1} & {a}_{3,2}\\\\ \n",
    "  {a}_{4,1} & {a}_{4,2}\\\\ \n",
    "\\end{bmatrix} = \\begin{pmatrix} \n",
    "  {a}_{1,1} & {a}_{1,2}\\\\ \n",
    "  {a}_{2,1} & {a}_{2,2}\\\\ \n",
    "  {a}_{3,1} & {a}_{3,2}\\\\ \n",
    "  {a}_{4,1} & {a}_{4,2}\\\\ \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Where ${a}_{i,j}$ (no brackets/parentheses) refers to a single element in the matrix, where $i$ refers to the row and $j$ refers to the column\n",
    "\n",
    "* Sometimes, we'll talk about *transposing* a matrix, which means we flip it along it's diagonal axis! When transposing a matrix, we swap the values of $m$ and $n$ and move the elements of the matrix to match. We refer to the transposed version of a matrix by adding a superscript of $T$ to it. For example\n",
    "<center><img src=\"img/Matrix_transpose.gif\" alt=\"Animation of a matrix being transposed\"/></center>\n",
    "$$ \n",
    "A = [a_{3,2}] = \\begin{bmatrix} \n",
    "  1 & 2\\\\ \n",
    "  3 & 4\\\\ \n",
    "  5 & 6\\\\  \n",
    "\\end{bmatrix} \\quad  A^{T} = [a^{T}_{2,3}] = \\begin{bmatrix} \n",
    "  1 & 3 & 5 \\\\ \n",
    "  2 & 4 & 6 \\\\ \n",
    "\\end{bmatrix}\n",
    "$$ \n",
    "\n",
    "\n",
    "* A 1-Dimensional matrix (where either $n=1$ or $m=1$) can also be called a **row vector** (when shape = $m \\times 1$) or a **column vector** (when shape = $1 \\times n$), and transposing one kind turns it into the other. These can be _treated_ like vectors (such as when you're performing certain operations), but are still matricies. A key point is that a *vector* **can not** be transposed, but a *matrix* can. (Yes this is confusing, this distinction won't come up today, but is important with Linear Algebra in general) \n",
    "$$\n",
    "A = [a_{4,1}] = \\begin{bmatrix} \n",
    "  1 \\\\ \n",
    "  2 \\\\ \n",
    "  3 \\\\ \n",
    "  4 \\\\\n",
    "  \\end{bmatrix} \\quad B = [b_{1,4}] = \\begin{bmatrix} \n",
    "  5 & 6 & 7 & 8\\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* **Importantly,** you can/will see some matricies represented in the form of a *column vector*, functioning as a ***table*** of vectors. You have to be careful in this distinction, as it's shorthand that needs to be expanded before performing operations on it as a matrix. For example, if we have a matrix $A = [a_{4,3}]$, we can write it as a table of 2-Dimensional vectors $\\vec{a_n} = (x,y)$ like so:\n",
    "\n",
    "$$\n",
    "A = [a_{4,2}] = \\begin{bmatrix} \n",
    "  \\vec{a_1} \\\\ \n",
    "  \\vec{a_2} \\\\ \n",
    "  \\vec{a_3} \\\\ \n",
    "  \\vec{a_4} \\\\\n",
    "  \\end{bmatrix} = \\begin{bmatrix} \n",
    "  {a_1}_x & {a_1}_y\\\\ \n",
    "  {a_2}_x & {a_2}_y\\\\ \n",
    "  {a_3}_x & {a_3}_y\\\\ \n",
    "  {a_4}_x & {a_4}_y\\\\ \n",
    "  \\end{bmatrix} = \\begin{bmatrix} \n",
    "  {a}_{1,1} & {a}_{1,2}\\\\ \n",
    "  {a}_{2,1} & {a}_{2,2}\\\\ \n",
    "  {a}_{3,1} & {a}_{3,2}\\\\ \n",
    "  {a}_{4,1} & {a}_{4,2}\\\\ \n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Dot Product\n",
    "The dot product of two matricies is simply the sum of each corresponding element multiplied together, defined as such:\n",
    "\n",
    "$ \\mathbf a \\cdot \\mathbf b = \\sum_{i=1}^n a_i b_i = a_1 b_1 + a_2 b_2 + \\cdots + a_n b_n $\n",
    "\n",
    "\n",
    "\n",
    "***Variable/Function Definitions - ML/Notebook Specific***\n",
    "Most of these symbols are common across Machine Learning/Neural Network literature, but there still might be slight variations. For the purposes of this notebook, these variables are defined as follows\n",
    "\n",
    "Dataset Related Variables:\n",
    "* $\\vec{x}$ - An **unmodified** *input* vector, these make up the inputs to our neural network\n",
    "* $\\vec{x_n}$ - pronounced \"*x n*\", e.g. \"x 1\", \"x 2\", etc. - a specific input matrix (feature), e.g. $\\vec{x_1}$, $\\vec{x_2}$, etc. (this also applies to other forms of $\\vec{x}$, $\\vec{y}$, and $\\vec{w}$ such as $\\vec{x'}$ and $\\vec{\\hat{y}}$ ) \n",
    "* $\\vec{x'}$ - pronounced *\"x prime\"* - Generally a modified value of x (derivitive of, though not speciifcally in the Calculus sense :) ), usually referring to the output of an intermediete step during pre-processing, an operation within a *hidden layer* of a neural network, or the output of the *hidden layer* itself\n",
    "* $\\vec{y}$ - A truth vector, containing whatever the _actual truth values_ that the neural network is trying to learn to replicate. \n",
    "* $\\vec{\\hat{y}}$ - pronounced *Y Hat* - the **modified**/**final** output vector of our neural network, usually after we apply an *activation function*\n",
    "* ${X}$ - (Capital/Uppercase X) - the _input_ matrix for our neural network, comprised of all input vectors ($x$)\n",
    "* $\\hat{Y}$ - (Capital/Uppercase Y Hat) - the _output_ matrix for our neural network, comprised of all output vectors ($\\hat{y}$) from our neutral network. \n",
    "\n",
    "Neural Network Related Variables/Functions:\n",
    "* $\\vec{w}$ - a *weight* vector, a _learned parameter_ that a neural network learns during the training process.  \n",
    "* $b$ - a *bias* scalar, a _learned parameter_ that a neural network learns during the training process.\n",
    "* $g(...)$ - an *activation function*, used to add *non linear behavior* to the network. This can represent one of multiple different functions (such as Step, ReLU, Sigmoid, TanH, etc. see the \"Activation Functions\" link in *Other Resources* above!), depending on the context and architecture of a given neural network. \n",
    "\n",
    "\n",
    "\n",
    "***Math Symbols/Definitions***\n",
    "This notation is pretty general and shared across most math literature you'll find, but it's important to check specific meanings regardless, **ESPECIALLY** if this notation is being used in other contexts/fields (Outside ML, Discrete Math, and Linear Algebra) \n",
    "* $\\in$ - pronounced as \"in\" - A symbol denoting that the value of a mentioned variable (usually $x$, $y$, etc.) exists within/is bounded by some set of numbers, meaning that it will _always_ be within that set and never be anything that's not part of it. \n",
    "* $\\{a, b, c\\}$ - pronounced \"set of ...\" - This is specificlly defined set (list) of numbers, usually used in conjunction with the above \"in\"/ $\\in$ symbol. (Note the curly braces $\\{ \\}$) \n",
    "* $\\mathbb{R}$ - pronounced \"\\<the set of> all real numbers\" - This symbol represents the set of **all** real numbers, usually used in conjunction with the above \"in\"/ $\\in$ symbol. \n",
    "* In this notebook, we also use the notation $\\mathbb{R}^d$ or $\\{a, b, c\\}^d$  , where $d$ indicates the dimensions/shape of whatever the variable we're referencing (Outside of this notebook, $\\mathbb{R}^n$ is often the same meaning where $d$ = $n$). \n",
    "    * e.g. $\\hat{Y} \\in \\mathbb{R}^{4 \\times 1}$ means that a ***matrix*** $\\hat{Y}$ in the shape of \"${4 \\times 1}$\", where each element of the matrix is a real number (part of the set $\\mathbb{R}$) \n",
    "    * e.g. $\\vec{x} \\in \\mathbb{R}^2$ is a ***vector*** $\\vec{x}$ in $2$ dimensional space, where the possible values of the vector's *components* are all real numbers. \n",
    "    * e.g. $\\vec{x} \\in \\{0,1\\}^2$ is a ***vector*** $\\vec{x}$ in $2$ dimensional space, where the possible values of the vector's *components* are either $0$ or $1$.  \n",
    "    * e.g. $\\vec{\\hat{y}} \\in \\{0,1\\}$ is a ***vector*** $\\vec{\\hat{y}}$ in $1$ dimensional space, where the possible values of the vector's *component* is either $0$ or $1$ \n",
    "    * e.g. $\\{0,1\\}^2$ is a ***vector*** (that's unnamed) in $2$ dimensional space, where the possible values of the vector's *components* are either $0$ or $1$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Networks\n",
    "\n",
    "This notebook explains various ways of implementing single-layer and multi-layer neural networks. The implementations are arranged by concrete (explicit) to abstract order so that one can understand the process black-boxed by deep learning frameworks.\n",
    "\n",
    "\n",
    "## Example Task: Boolean Logic Gates\n",
    "In order to focus on explaining the internals of training, this notebook uses a simple and classic example: *boolean logic gates* (aka *threshold logic units*).\n",
    "\n",
    "When we talk about boolean logic, we refer to operations that exclusively use ***Truth Values***, which are *binary* values that can **only** be *True* or *False*, typically represented where $x=0$ means *False* and $x=1$ means *True*. \n",
    "\n",
    "\n",
    "*Boolean Logic Gates* take one or more boolean values as input and produce a single boolean output. Some basic *Boolean Logic Gates* are:\n",
    "* AND ($\\wedge$) - Takes two inputs (A & B) and outputs *True* **if and only if** both A and B are True. Otherwise outputs *False*.\n",
    "* OR ($\\vee$) - Takes two inputs (A & B) and outputs *True* **if either** A or B are *True*. Otherwise outputs *False*.\n",
    "* NOT ($\\lnot$) - Takes one input (A) and *inverts* it. if A is True, it outputs *False*. If A is False, it outputs *True*.\n",
    "* NAND ($\\uparrow$) - Takes two inputs (A & B) and outputs *True* **if only both outputs are the not true* A and B are not. Otherwise outputs *False*.\n",
    "    * This is the opposite of the AND Gate, effectivly as if you take the output of an AND gate and pass it through a NOT gate\n",
    "* NOR ($\\downarrow$) - Takes two inputs (A & B) and outputs *True* **if** A or B are *False*. Otherwise outputs *False*.\n",
    "    * This is the opposite of the NOR Gate, effectivly as if you take the output of an OR gate and pass it through a NOT gate\n",
    "* XOR ($\\oplus$) - *exclusive OR* - Takes two inputs (A & B) and outputs *True* if **only one input* (A or B, but not both at the same time) is *True*. Otherwise outputs *False*.\n",
    "* XNOR ($\\odot$) - *exclusive NOR* - Takes two inputs (A & B) and outputs *True* if **only both outputs are the same*. Otherwise outputs *False*.\n",
    "    * This is the opposite of the XOR Gate, effectivly as if you take the output of an XOR gate and pass it through a NOT gate\n",
    "<center><img src=\"img/logic_gates.png\" alt=\"Logic gate symbols, as typically used in electronics\"/></center>\n",
    "\n",
    "It's common to compile these operations into a \"truth table\", like below. Columns $A$ and $B$ represent the inputs, and the name of the *Boolean Logic Gates* represents the output for the given values of $A$ and $B$ in a row. \n",
    "| $A$ | $B$ | AND | OR | NOT* | NAND | NOR | XOR | XNOR |\n",
    "| :-: | :-: | :---: | :--: | :---: | :----: | :---: | :---: | :----: |\n",
    "| 0 | 0 | 0 | 0 | 1 | 1 | 1 | 0 | 1 |\n",
    "| 0 | 1 | 0 | 1 | 1 | 1 | 0 | 1 | 0 |\n",
    "| 1 | 0 | 0 | 1 | 0 | 1 | 0 | 1 | 0 |\n",
    "| 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 1 |\n",
    "\n",
    "\\* Only uses one input, column A\n",
    "\n",
    "\n",
    "Defining $x=0$ as *false* and $x=1$ as *true*, single-layer neural networks can realize logic units such as AND ($\\wedge$), OR ($\\vee$), NOT ($\\lnot$), and their inverted counterparts. \n",
    "\n",
    "Because they're only one layer, they're unable to represent logical compounds such as XOR/XNOR. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qxdFG8U2Net8"
   },
   "source": [
    "## Using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ug4zqMhLB-B6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ea5cM4JEsENr"
   },
   "source": [
    "### Single-layer perceptron\n",
    "\n",
    "A single layer perceptron predicts a binary label $\\hat{y} \\in \\{0, 1\\}$ for a given input vector $\\boldsymbol{x} \\in \\mathbb{R}^d$ ($d$ presents the number of dimensions of inputs) by using the following formula,\n",
    "$$\n",
    "\\hat{y} = g(\\boldsymbol{w} \\cdot \\boldsymbol{x} + b) = g(w_1 x_1 + w_2 x_2 + ... + w_d x_d + b)\n",
    "$$\n",
    "\n",
    "Here, $\\boldsymbol{w} \\in \\mathbb{R}^d$ is a **weight** (vector) ; $b \\in \\mathbb{R}$ is a **bias** (scalar) ; and $g(.)$ denotes an **activation function** (in this case, that's the Unit Step Function) (we assume $g(0)=0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kRpaNDA8BWJY"
   },
   "source": [
    "For simplicity, let us consider examples with two-dimensional inputs ($d=2$).\n",
    "We can represent an input vector $\\boldsymbol{x} \\in \\mathbb{R}^2$ and weight vector $\\boldsymbol{w} \\in \\mathbb{R}^2$ with `numpy.array`. We also define the bias term $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kyX1MfRvBD25"
   },
   "outputs": [],
   "source": [
    "# Define some weight, input, and bias values to use \n",
    "x = np.array([0, 1]) # 3 dimensional vector with possible values {0,1} - Inputs\n",
    "w = np.array([1.0, 1.0]) # 2 dimensional vector with possible values ℝ    - Weights\n",
    "b = 1.0 # scalar value                                                    - Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P-P2AkbTCI0P"
   },
   "source": [
    "The following code computes $\\boldsymbol{w} \\cdot \\boldsymbol{x} + b$,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "4g_115nLCGqs",
    "outputId": "70e0b595-0d7d-4a6c-9a6f-f667cafb75bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " np.dot(x, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UUzhy8YFDIuf"
   },
   "source": [
    "We can apply the step function (also known as a Heaviside or Unit step function) as an *activation function*, $g()$:\n",
    "\n",
    "<img src=\"img/heaviside_step.png\" alt=\"Plot of the Heaviside Step Function\" style=\"background-color:white; width: 400px;\"/>\n",
    "When applied to the above result as $g(\\boldsymbol{w} \\cdot \\boldsymbol{x} + b)$, it yields a binary label, $\\hat{y}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "KV6a5JT5DH6c",
    "outputId": "5302b46b-a1ab-4996-dccc-42641e546599"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.heaviside(np.dot(x, w) + b, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Ppo_Jh8EEs3"
   },
   "source": [
    "#### Including the bias term into the weight vector\n",
    "\n",
    "For concise implementation, we include a bias term `b` as an additional dimension to the weight vector `w`. More concretely, we append an element with the value of $1$ to each input,\n",
    "$$\n",
    "\\boldsymbol{x} = (0, 1) \\rightarrow \\boldsymbol{x}' = (0, 1, 1)\n",
    "$$\n",
    "and expand the dimension of the weight vector $\\boldsymbol{w} \\in \\mathbb{R}^{3}$.\n",
    "\n",
    "Then, the formula of the single-layer perceptron becomes,\n",
    "$$\n",
    "\\hat{y} = g((w_1, w_2, w_3) \\cdot \\boldsymbol{x}') = g(w_1 x_1 + w_2 x_2 + w_3)\n",
    "$$\n",
    "In other words, $w_1$ and $w_2$ represent weights for $x_1$ and $x_2$, respectively, and $w_3$ is our bias value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lmQGDVM2FnGy"
   },
   "outputs": [],
   "source": [
    "x = np.array([0, 1, 1]) \n",
    "w = np.array([1.0, 1.0, 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "53zn1dWQFvLw"
   },
   "source": [
    "We can simplify the code to predict a binary label $\\hat{y}$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "rsgq_oOzF4PZ",
    "outputId": "b6b675f1-2a16-4a8e-900d-7131164449a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.heaviside(np.dot(x, w), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "53SDTFENBA19"
   },
   "source": [
    "#### Training a NAND gate\n",
    "\n",
    "Let's train a NAND gate with two inputs. More specifically, we want to find a weight vector $\\boldsymbol{w}$ and a bias value $b$ of a single-layer perceptron that realizes the truth table of the NAND gate: $\\{0,1\\}^2 \\to \\{0,1\\}$.\n",
    "\n",
    "| $x_1$ | $x_2$ | $y$ |\n",
    "| :---: |:---: | :---: |\n",
    "| 0 | 0 | 1 |\n",
    "| 0 | 1 | 1 |\n",
    "| 1 | 0 | 1 |\n",
    "| 1 | 1 | 0 |\n",
    "\n",
    "We convert the truth table into a training set consisting of all mappings of the NAND gate,\n",
    "$$\n",
    "\\boldsymbol{x}_1 = (0, 0), y_1 = 1 \\\\\n",
    "\\boldsymbol{x}_2 = (0, 1), y_2 = 1 \\\\\n",
    "\\boldsymbol{x}_3 = (1, 0), y_3 = 1 \\\\\n",
    "\\boldsymbol{x}_4 = (1, 1), y_4 = 0 \\\\\n",
    "$$\n",
    "\n",
    "As explained earlier, we include the bias term into the last dimension.\n",
    "$$\n",
    "\\boldsymbol{x}'_1 = (0, 0, 1), y_1 = 1 \\\\\n",
    "\\boldsymbol{x}'_2 = (0, 1, 1), y_2 = 1 \\\\\n",
    "\\boldsymbol{x}'_3 = (1, 0, 1), y_3 = 1 \\\\\n",
    "\\boldsymbol{x}'_4 = (1, 1, 1), y_4 = 0 \\\\\n",
    "$$\n",
    "\n",
    "The code below implements Rosenblatt's perceptron algorithm with a fixed number of iterations (50 times). We use a constant learning rate 0.5 for simplicity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 917
    },
    "colab_type": "code",
    "id": "2ygoUjQYrPoj",
    "outputId": "7f84944b-3ee0-4475-e20f-783ef614c4c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 : training sample index=2, x=[1 0 1], w=[0. 0. 0.], y=1.0, y_pred=0.0, y_err=1.0\n",
      "#1 : training sample index=3, x=[1 1 1], w=[0.5 0.  0.5], y=0.0, y_pred=1.0, y_err=-1.0\n",
      "#2 : training sample index=0, x=[0 0 1], w=[ 0.  -0.5  0. ], y=1.0, y_pred=0.0, y_err=1.0\n",
      "#3 : training sample index=3, x=[1 1 1], w=[ 0.  -0.5  0.5], y=0.0, y_pred=0.0, y_err=0.0\n",
      "#4 : training sample index=1, x=[0 1 1], w=[ 0.  -0.5  0.5], y=1.0, y_pred=0.0, y_err=1.0\n",
      "#5 : training sample index=2, x=[1 0 1], w=[0. 0. 1.], y=1.0, y_pred=1.0, y_err=0.0\n",
      "#6 : training sample index=3, x=[1 1 1], w=[0. 0. 1.], y=0.0, y_pred=1.0, y_err=-1.0\n",
      "#7 : training sample index=3, x=[1 1 1], w=[-0.5 -0.5  0.5], y=0.0, y_pred=0.0, y_err=0.0\n",
      "#8 : training sample index=2, x=[1 0 1], w=[-0.5 -0.5  0.5], y=1.0, y_pred=0.0, y_err=1.0\n",
      "#9 : training sample index=1, x=[0 1 1], w=[ 0.  -0.5  1. ], y=1.0, y_pred=1.0, y_err=0.0\n",
      "#10: training sample index=1, x=[0 1 1], w=[ 0.  -0.5  1. ], y=1.0, y_pred=1.0, y_err=0.0\n",
      "#11: training sample index=1, x=[0 1 1], w=[ 0.  -0.5  1. ], y=1.0, y_pred=1.0, y_err=0.0\n",
      "#12: training sample index=0, x=[0 0 1], w=[ 0.  -0.5  1. ], y=1.0, y_pred=1.0, y_err=0.0\n",
      "#13: training sample index=0, x=[0 0 1], w=[ 0.  -0.5  1. ], y=1.0, y_pred=1.0, y_err=0.0\n",
      "#14: training sample index=1, x=[0 1 1], w=[ 0.  -0.5  1. ], y=1.0, y_pred=1.0, y_err=0.0\n",
      "#15: training sample index=3, x=[1 1 1], w=[ 0.  -0.5  1. ], y=0.0, y_pred=1.0, y_err=-1.0\n",
      "#16: training sample index=2, x=[1 0 1], w=[-0.5 -1.   0.5], y=1.0, y_pred=0.0, y_err=1.0\n",
      "#17: training sample index=2, x=[1 0 1], w=[ 0. -1.  1.], y=1.0, y_pred=1.0, y_err=0.0\n",
      "#18: training sample index=1, x=[0 1 1], w=[ 0. -1.  1.], y=1.0, y_pred=0.0, y_err=1.0\n",
      "#19: training sample index=2, x=[1 0 1], w=[ 0.  -0.5  1.5], y=1.0, y_pred=1.0, y_err=0.0\n",
      "#20: training sample index=1, x=[0 1 1], w=[ 0.  -0.5  1.5], y=1.0, y_pred=1.0, y_err=0.0\n",
      "#21: training sample index=2, x=[1 0 1], w=[ 0.  -0.5  1.5], y=1.0, y_pred=1.0, y_err=0.0\n",
      "#22: training sample index=2, x=[1 0 1], w=[ 0.  -0.5  1.5], y=1.0, y_pred=1.0, y_err=0.0\n",
      "#23: training sample index=2, x=[1 0 1], w=[ 0.  -0.5  1.5], y=1.0, y_pred=1.0, y_err=0.0\n",
      "#24: training sample index=3, x=[1 1 1], w=[ 0.  -0.5  1.5], y=0.0, y_pred=1.0, y_err=-1.0\n",
      "#25: training sample index=0, x=[0 0 1], w=[-0.5 -1.   1. ], y=1.0, y_pred=1.0, y_err=0.0\n",
      "#26: training sample index=2, x=[1 0 1], w=[-0.5 -1.   1. ], y=1.0, y_pred=1.0, y_err=0.0\n",
      "#27: training sample index=1, x=[0 1 1], w=[-0.5 -1.   1. ], y=1.0, y_pred=0.0, y_err=1.0\n",
      "#28: training sample index=1, x=[0 1 1], w=[-0.5 -0.5  1.5], y=1.0, y_pred=1.0, y_err=0.0\n",
      "#29: training sample index=2, x=[1 0 1], w=[-0.5 -0.5  1.5], y=1.0, y_pred=1.0, y_err=0.0\n",
      "#30: training sample index=1, x=[0 1 1], w=[-0.5 -0.5  1.5], y=1.0, y_pred=1.0, y_err=0.0\n",
      "#31: training sample index=2, x=[1 0 1], w=[-0.5 -0.5  1.5], y=1.0, y_pred=1.0, y_err=0.0\n",
      "#32: training sample index=0, x=[0 0 1], w=[-0.5 -0.5  1.5], y=1.0, y_pred=1.0, y_err=0.0\n",
      "#33: training sample index=2, x=[1 0 1], w=[-0.5 -0.5  1.5], y=1.0, y_pred=1.0, y_err=0.0\n",
      "#34: training sample index=3, x=[1 1 1], w=[-0.5 -0.5  1.5], y=0.0, y_pred=1.0, y_err=-1.0\n",
      "#35: training sample index=2, x=[1 0 1], w=[-1. -1.  1.], y=1.0, y_pred=0.0, y_err=1.0\n",
      "#36: training sample index=0, x=[0 0 1], w=[-0.5 -1.   1.5], y=1.0, y_pred=1.0, y_err=0.0\n",
      "#37: training sample index=1, x=[0 1 1], w=[-0.5 -1.   1.5], y=1.0, y_pred=1.0, y_err=0.0\n",
      "#38: training sample index=2, x=[1 0 1], w=[-0.5 -1.   1.5], y=1.0, y_pred=1.0, y_err=0.0\n",
      "#39: training sample index=2, x=[1 0 1], w=[-0.5 -1.   1.5], y=1.0, y_pred=1.0, y_err=0.0\n",
      "#40: training sample index=0, x=[0 0 1], w=[-0.5 -1.   1.5], y=1.0, y_pred=1.0, y_err=0.0\n",
      "#41: training sample index=0, x=[0 0 1], w=[-0.5 -1.   1.5], y=1.0, y_pred=1.0, y_err=0.0\n",
      "#42: training sample index=1, x=[0 1 1], w=[-0.5 -1.   1.5], y=1.0, y_pred=1.0, y_err=0.0\n",
      "#43: training sample index=3, x=[1 1 1], w=[-0.5 -1.   1.5], y=0.0, y_pred=0.0, y_err=0.0\n",
      "#44: training sample index=3, x=[1 1 1], w=[-0.5 -1.   1.5], y=0.0, y_pred=0.0, y_err=0.0\n",
      "#45: training sample index=3, x=[1 1 1], w=[-0.5 -1.   1.5], y=0.0, y_pred=0.0, y_err=0.0\n",
      "#46: training sample index=3, x=[1 1 1], w=[-0.5 -1.   1.5], y=0.0, y_pred=0.0, y_err=0.0\n",
      "#47: training sample index=1, x=[0 1 1], w=[-0.5 -1.   1.5], y=1.0, y_pred=1.0, y_err=0.0\n",
      "#48: training sample index=1, x=[0 1 1], w=[-0.5 -1.   1.5], y=1.0, y_pred=1.0, y_err=0.0\n",
      "#49: training sample index=1, x=[0 1 1], w=[-0.5 -1.   1.5], y=1.0, y_pred=1.0, y_err=0.0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Training data for NAND.\n",
    "x = np.array([\n",
    "    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n",
    "    ])\n",
    "y = np.array([1., 1., 1., 0])\n",
    "w = np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "lr = 0.5 #Learning Rate\n",
    "for t in range(50):\n",
    "    # Pick one random sample of traing data, at index (i), at random.\n",
    "    i = random.choice(range(len(y)))\n",
    "    # Predict the label for the instance x[i] with the current parameter w.\n",
    "    y_pred = np.heaviside(np.dot(x[i], w), 0)\n",
    "    # Show the detail of the instance and the current parameter.\n",
    "    print(f'#{t:<2}: training sample index={i}, x={x[i]}, w={w}, y={y[i]}, y_pred={y_pred}, y_err={y[i] - y_pred}')\n",
    "    # Update the parameter.\n",
    "    loss = y[i] - y_pred\n",
    "    w += loss * lr * x[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8geTVVpnlu1O"
   },
   "source": [
    "We can confirm the learned parameter and classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "TYoeshu2rXdK",
    "outputId": "1199ece8-3d2c-4521-b473-ceaa3d2910e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5, -1. ,  1.5])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "hOFgUFojraFA",
    "outputId": "dd1c5c9b-67b2-48a7-a468-a87d20eea67d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Truth: [1. 1. 1. 0.]\n",
      "  Pred: [1. 1. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_pred=np.heaviside(np.dot(x, w), 0)\n",
    "print(f\" Truth: {y}\")\n",
    "print(f\"  Pred: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bl_ZAEguNzgU"
   },
   "source": [
    "### Single-layer perceptron with batching\n",
    "\n",
    "When training a model using a larger dataset with many samples, it's ideal to reduce the number of computations you perform - especially when running native python code (which is a lot slower than other languages). The common technique to speed up a machine-learning code written in Python is to run code using libraries that accelerate the large matrix operations, such as numpy (or Tensorflow, Keras, and Pytorch, but we'll get to that in a bit!) \n",
    "\n",
    "Even when using a libaray to accelerate these operations, we'll still run into issues with the sheer number of computations if we calculated the loss and updated the weights for every single image, so instead we use a technique called **batching**, where we calculate the loss of multiple images at a time and update the weights once every $n$ images, where $n$ is the number of images in one batch, or the **batch size**\n",
    "\n",
    "So, putting that into practice;\n",
    "\n",
    "The single-layer perceptron makes predictions for four inputs, \n",
    "$$\n",
    "\\hat{y}_1 = g(\\boldsymbol{x}_1 \\cdot \\boldsymbol{w}) \\\\\n",
    "\\hat{y}_2 = g(\\boldsymbol{x}_2 \\cdot \\boldsymbol{w}) \\\\\n",
    "\\hat{y}_3 = g(\\boldsymbol{x}_3 \\cdot \\boldsymbol{w}) \\\\\n",
    "\\hat{y}_4 = g(\\boldsymbol{x}_4 \\cdot \\boldsymbol{w}) \\\\\n",
    "$$\n",
    "and if we give it 4 inputs, we'll get an output for each input\n",
    "$$\n",
    "\\hat{Y} = \\begin{pmatrix} \n",
    "  \\hat{y}_1 \\\\ \n",
    "  \\hat{y}_2 \\\\ \n",
    "  \\hat{y}_3 \\\\ \n",
    "  \\hat{y}_4 \\\\ \n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "\n",
    "Here, we define $\\hat{Y} \\in \\mathbb{R}^{4 \\times 1}$ and $X \\in \\mathbb{R}^{4 \\times d}$ as,\n",
    "$$\n",
    "\\hat{Y} = \\begin{pmatrix} \n",
    "  \\hat{y}_1 \\\\ \n",
    "  \\hat{y}_2 \\\\ \n",
    "  \\hat{y}_3 \\\\ \n",
    "  \\hat{y}_4 \\\\ \n",
    "\\end{pmatrix},\n",
    "X = \\begin{pmatrix} \n",
    "  \\boldsymbol{x}_1 \\\\ \n",
    "  \\boldsymbol{x}_2 \\\\ \n",
    "  \\boldsymbol{x}_3 \\\\ \n",
    "  \\boldsymbol{x}_4 \\\\ \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Then, we can write the four predictions in one dot-product computation,\n",
    "$$\n",
    "\\hat{Y} = X \\cdot \\boldsymbol{w}\n",
    "$$\n",
    "\n",
    "The code below implements this idea. The function `np.heaviside()` yields a vector corresponding to the four predictions, applying the step function for every element of the argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "mD7f0rdz4wxS",
    "outputId": "1afae4dc-0b1f-4dbb-edfe-ca7cf1e7c733"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 1.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n",
    "    ])\n",
    "w = np.array([1.0, 0.5, -0.5])\n",
    "np.heaviside(np.dot(x, w), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HfwL6Z8q5Cyu"
   },
   "source": [
    "The code below applies the Perceptron algorithm with batching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "2fK-_WimtPwb",
    "outputId": "658fa734-c537-4801-9b17-9c046d76e833"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0: w=[0. 0. 0.], Y=[1 1 1 0], Ypred=[0. 0. 0. 0.], Yerr=[1. 1. 1. 0.], dw=[1. 1. 3.]\n",
      "[0. 0. 0. 0.]\n",
      "#1: w=[0.5 0.5 1.5], Y=[1 1 1 0], Ypred=[1. 1. 1. 1.], Yerr=[ 0.  0.  0. -1.], dw=[-1. -1. -1.]\n",
      "[0. 0. 0. 0.]\n",
      "#2: w=[0. 0. 1.], Y=[1 1 1 0], Ypred=[1. 1. 1. 1.], Yerr=[ 0.  0.  0. -1.], dw=[-1. -1. -1.]\n",
      "[0. 0. 0. 0.]\n",
      "#3: w=[-0.5 -0.5  0.5], Y=[1 1 1 0], Ypred=[1. 0. 0. 0.], Yerr=[0. 1. 1. 0.], dw=[1. 1. 2.]\n",
      "[0. 0. 0. 0.]\n",
      "#4: w=[0.  0.  1.5], Y=[1 1 1 0], Ypred=[1. 1. 1. 1.], Yerr=[ 0.  0.  0. -1.], dw=[-1. -1. -1.]\n",
      "[0. 0. 0. 0.]\n",
      "#5: w=[-0.5 -0.5  1. ], Y=[1 1 1 0], Ypred=[1. 1. 1. 0.], Yerr=[0. 0. 0. 0.], dw=[0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "#6: w=[-0.5 -0.5  1. ], Y=[1 1 1 0], Ypred=[1. 1. 1. 0.], Yerr=[0. 0. 0. 0.], dw=[0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "#7: w=[-0.5 -0.5  1. ], Y=[1 1 1 0], Ypred=[1. 1. 1. 0.], Yerr=[0. 0. 0. 0.], dw=[0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "#8: w=[-0.5 -0.5  1. ], Y=[1 1 1 0], Ypred=[1. 1. 1. 0.], Yerr=[0. 0. 0. 0.], dw=[0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "#9: w=[-0.5 -0.5  1. ], Y=[1 1 1 0], Ypred=[1. 1. 1. 0.], Yerr=[0. 0. 0. 0.], dw=[0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Training data for NAND.\n",
    "x = np.array([\n",
    "    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n",
    "    ])\n",
    "y = np.array([1, 1, 1, 0])\n",
    "w = np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "lr = 0.5\n",
    "for t in range(10):\n",
    "    y_pred = np.heaviside(np.dot(x, w), 0) # Instead of picking a single image to calculate with, we give it 4 at once\n",
    "    print(f'#{t}: w={w}, Y={y}, Ypred={y_pred}, Yerr={y-y_pred}, dw={np.dot((y - y_pred), x)}')\n",
    "    Yerr = (y - y_pred) #loss is now a 4 dimensional vector - one value for each input from x\n",
    "    print(loss)\n",
    "    w += lr * np.dot(Yerr, x) # update the weight parameters, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CWvR709WnUlH"
   },
   "source": [
    "We can confirm the learned parameters and classification results match what we expect/the example done without batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "_x4p1BldtQ-K",
    "outputId": "e573289d-1d8f-41a3-b2d2-7c97cbf60c15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5, -0.5,  1. ])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "E-P2RpWrtVyf",
    "outputId": "5efd5ddb-a532-42cb-9b97-c66841fa46f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Truth: [1 1 1 0]\n",
      "  Pred: [1. 1. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_pred=np.heaviside(np.dot(x, w), 0)\n",
    "print(f\" Truth: {y}\")\n",
    "print(f\"  Pred: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nkxBcCSTtDvm"
   },
   "source": [
    "### Stochastic gradient descent (SGD) with mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bltpfNRctjV5",
    "outputId": "b05e89c0-4271-4b28-a1e3-36028534012a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(v):\n",
    "    return 1.0 / (1 + np.exp(-v))\n",
    "\n",
    "# Training data for NAND.\n",
    "x = np.array([\n",
    "    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n",
    "    ])\n",
    "y = np.array([1, 1, 1, 0])\n",
    "w = np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "eta = 0.5\n",
    "for t in range(100):\n",
    "    y_pred = sigmoid(np.dot(x, w))\n",
    "    print(f'#{t}: w={w}, Y={y}, Ypred={y_pred}, Yerr={y-y_pred}, dw={np.dot((y - y_pred), x)}')\n",
    "    w -= eta * np.dot((y_pred - y), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "9bASDMfhtm-I",
    "outputId": "44625ba1-73bc-4b26-9519-4eb0154a580e"
   },
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "-69r0c4KtqlW",
    "outputId": "2af491eb-8c1f-4926-ddcc-ced95421a0c5"
   },
   "outputs": [],
   "source": [
    "sigmoid(np.dot(x, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ELUeNFRFuJv3"
   },
   "source": [
    "## Automatic differentiation\n",
    "\n",
    "#todo explain autodif anf usefulness\n",
    "\n",
    "\n",
    "Consider a loss function,\n",
    "$$\n",
    "l_{\\boldsymbol{x}}(\\boldsymbol{w}) = - \\log \\sigma(\\boldsymbol{w} \\cdot \\boldsymbol{x}) = - \\log \\frac{1}{1 + e^{-\\boldsymbol{w} \\cdot \\boldsymbol{x}}}\n",
    "$$\n",
    "\n",
    "This section shows implementations in different libraries of deep learning for computing the loss value $l_{\\boldsymbol{x}}(\\boldsymbol{w})$ and gradients $\\frac{\\partial l_{\\boldsymbol{x}}(\\boldsymbol{w})}{\\partial \\boldsymbol{w}}$ when $\\boldsymbol{x} = (1, 1, 1)$ and $\\boldsymbol{w} = (1, 1, -1.5)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aB0DwVOyuXP_"
   },
   "source": [
    "### Using autograd\n",
    "\n",
    "See: https://github.com/HIPS/autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "DyC9iOFO-1cd",
    "outputId": "04582ded-1615-4954-c0d2-2d6925fbf5ed"
   },
   "outputs": [],
   "source": [
    "import autograd\n",
    "import autograd.numpy as np\n",
    "\n",
    "def loss(w, x):\n",
    "    return -np.log(1.0 / (1 + np.exp(-np.dot(x, w))))\n",
    "\n",
    "x = np.array([1, 1, 1])\n",
    "w = np.array([1.0, 1.0, -1.5])\n",
    "\n",
    "grad_loss = autograd.grad(loss)\n",
    "print(loss(w, x))\n",
    "print(grad_loss(w, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hZ_BH643pO9Y"
   },
   "source": [
    "### Using TensorFlow Eager\n",
    "\n",
    "See: https://www.tensorflow.org/guide/autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "Zp87EyeAp6CY",
    "outputId": "ea22eb60-add8-4bb8-97fb-88334f1be00e"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "dtype = tf.float32\n",
    "\n",
    "x = tf.constant([1, 1, 1], dtype=dtype, name='x')\n",
    "w = tf.Variable([1.0, 1.0, -1.5], dtype=dtype, name='w')\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = -tf.math.log(tf.math.sigmoid(tf.tensordot(x, w, 1)))\n",
    "\n",
    "print(loss.numpy())\n",
    "print(tape.gradient(loss, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OGS23bDSazMJ"
   },
   "source": [
    "### Single-layer neural network with high-level NN modules (w/ optimizers)\n",
    "\n",
    "Rewrite for Keras!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jt9eizLFo1iN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "# Training data for NAND.\n",
    "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n",
    "y = torch.tensor([[1], [1], [1], [0]], dtype=dtype)\n",
    "                                        \n",
    "# Define a neural network using high-level modules.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 1, bias=True),   # 2 dims (with bias) -> 1 dim\n",
    ")\n",
    "\n",
    "# Binary corss-entropy loss after sigmoid function.\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "\n",
    "# Used for plotting loss values.\n",
    "loss_history = []\n",
    "\n",
    "eta = 0.5\n",
    "for t in range(100):\n",
    "    y_pred = model(x)                   # Make predictions.\n",
    "    loss = loss_fn(y_pred, y)           # Compute the loss.\n",
    "\n",
    "    loss_history.append(loss.item())    # Record the loss value.\n",
    "    #print(f'#{t}: loss={loss.item()}')\n",
    "    \n",
    "    model.zero_grad()                   # Zero-clear the gradients.\n",
    "    loss.backward()                     # Compute the gradients.\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= eta * param.grad   # Update the parameters using SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "D20KVsRJ5wJK",
    "outputId": "bceabb25-a1c2-4671-dea0-61ae20e211d4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Iteration #')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "Zq6oqLmIENFa",
    "outputId": "18be6cc1-2a61-4a3f-a337-b553c9509a2f"
   },
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "QdPOdNgO840b",
    "outputId": "9cdadee7-094f-4358-9720-7c0b9eedeed2"
   },
   "outputs": [],
   "source": [
    "model(x).sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KfuoJMeqbClA"
   },
   "source": [
    "# Multi-layer neural network with high-level NN modules\n",
    "\n",
    "introduce the idea of layers in a network\n",
    "\n",
    "do Jet Tagger stuff\n",
    "\n",
    "based on [the hls4ml tutorial part 1](https://github.com/fastmachinelearning/hls4ml-tutorial/blob/main/part1_getting_started.ipynb)\n",
    "\n",
    "TODO: Some of this is based on older sklearn/tf and needs to be updated to work properly...\n",
    "\n",
    "\n",
    "## Particle Physics Example: Jet Tagging\n",
    "\n",
    "<img src=\"img/jet_tagger_jets.png\" alt=\"2D Representations of the different kinds of particle jets the neural network will classify\" style=\"background-color:white; width: 800px;\"/>\n",
    "\n",
    "blah\n",
    "\n",
    "### Multi-Layer Perceptron - Your first Deep Neural Network!\n",
    "\n",
    "<img src=\"img/jet_tagger_mlp.png\" alt=\"Graph of the Jet Tagger MLP Neural Network\" style=\"background-color:white; width: 400px;\"/>\n",
    "\n",
    "blah\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-12 15:10:22.578969: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-07-12 15:10:22.579057: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "#TODO: stupid tf error in JupyterHub - does it fail in colab?\n",
    "'''\n",
    "TypeError: Descriptors cannot not be created directly.\n",
    "If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n",
    "If you cannot immediately regenerate your protos, some other possible workarounds are:\n",
    " 1. Downgrade the protobuf package to 3.20.x or lower.\n",
    " 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n",
    "\n",
    "More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n",
    "'''\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "data = jet_tagging_data\n",
    "X, y = data['data'], data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zlogz', 'c1_b0_mmdt', 'c1_b1_mmdt', 'c1_b2_mmdt', 'c2_b1_mmdt', 'c2_b2_mmdt', 'd2_b1_mmdt', 'd2_b2_mmdt', 'd2_a1_b1_mmdt', 'd2_a1_b2_mmdt', 'm2_b1_mmdt', 'm2_b2_mmdt', 'n2_b1_mmdt', 'n2_b2_mmdt', 'mass_mmdt', 'multiplicity']\n",
      "(830000, 16) (830000,)\n",
      "      zlogz  c1_b0_mmdt  c1_b1_mmdt  c1_b2_mmdt  c2_b1_mmdt  c2_b2_mmdt  \\\n",
      "0 -2.935125    0.383155    0.005126    0.000084    0.009070    0.000179   \n",
      "1 -1.927335    0.270699    0.001585    0.000011    0.003232    0.000029   \n",
      "2 -3.112147    0.458171    0.097914    0.028588    0.124278    0.038487   \n",
      "3 -2.666515    0.437068    0.049122    0.007978    0.047477    0.004802   \n",
      "4 -2.484843    0.428981    0.041786    0.006110    0.023066    0.001123   \n",
      "\n",
      "   d2_b1_mmdt  d2_b2_mmdt  d2_a1_b1_mmdt  d2_a1_b2_mmdt  m2_b1_mmdt  \\\n",
      "0    1.769445    2.123898       1.769445       0.308185    0.135687   \n",
      "1    2.038834    2.563099       2.038834       0.211886    0.063729   \n",
      "2    1.269254    1.346238       1.269254       0.246488    0.115636   \n",
      "3    0.966505    0.601864       0.966505       0.160756    0.082196   \n",
      "4    0.552002    0.183821       0.552002       0.084338    0.048006   \n",
      "\n",
      "   m2_b2_mmdt  n2_b1_mmdt  n2_b2_mmdt   mass_mmdt  multiplicity  \n",
      "0    0.083278    0.412136    0.299058    8.926882          75.0  \n",
      "1    0.036310    0.310217    0.226661    3.886512          31.0  \n",
      "2    0.079094    0.357559    0.289220  162.144669          61.0  \n",
      "3    0.033311    0.238871    0.094516   91.258934          39.0  \n",
      "4    0.014450    0.141906    0.036665   79.725777          35.0  \n",
      "0    g\n",
      "1    w\n",
      "2    t\n",
      "3    z\n",
      "4    w\n",
      "Name: class, dtype: category\n",
      "Categories (5, object): ['g', 'q', 'w', 'z', 't']\n"
     ]
    }
   ],
   "source": [
    "# look at some example data\n",
    "print(data['feature_names'])\n",
    "print(X.shape, y.shape)\n",
    "print(X[:5])\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "As it stands, our data isn't quite ready to feed into a network. We need to do a little bit of work ahead of time (**preprocessing**) to format the data and apply some statistical methods to make things easier for the network to understand.\n",
    "\n",
    "### Scaling Data\n",
    "\n",
    "blah\n",
    "\n",
    "### One-Hot Encoding\n",
    "\n",
    "blah\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (830000, 5) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m le \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[0;32m----> 2\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m to_categorical(y, \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      4\u001b[0m X_train_val, X_test, y_train_val, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/tiny-mlperf-env/lib/python3.8/site-packages/sklearn/preprocessing/_label.py:115\u001b[0m, in \u001b[0;36mLabelEncoder.fit_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, y):\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;124;03m\"\"\"Fit label encoder and return encoded labels.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m        Encoded labels.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcolumn_or_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_, y \u001b[38;5;241m=\u001b[39m _unique(y, return_inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/.conda/envs/tiny-mlperf-env/lib/python3.8/site-packages/sklearn/utils/validation.py:1156\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m   1147\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1148\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA column-vector y was passed when a 1d array was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1149\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected. Please change the shape of y to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   1153\u001b[0m         )\n\u001b[1;32m   1154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mravel(y)\n\u001b[0;32m-> 1156\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1157\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my should be a 1d array, got an array of shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(shape)\n\u001b[1;32m   1158\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (830000, 5) instead."
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y = to_categorical(y, 5)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_val = scaler.fit_transform(X_train_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing our MLP in Keras\n",
    "\n",
    "blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from callbacks import all_callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(16,), name='fc1', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "model.add(Activation(activation='relu', name='relu1'))\n",
    "model.add(Dense(32, name='fc2', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "model.add(Activation(activation='relu', name='relu2'))\n",
    "model.add(Dense(32, name='fc3', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "model.add(Activation(activation='relu', name='relu3'))\n",
    "model.add(Dense(5, name='output', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "model.add(Activation(activation='softmax', name='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our MLP\n",
    "\n",
    "blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D6ss25zA9nPk"
   },
   "outputs": [],
   "source": [
    "# Compile our model \n",
    "optimizer = Adam(lr=0.0001)\n",
    "model.compile(optimizer=optimizer, loss=['categorical_crossentropy'], metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train! \n",
    "model.fit(\n",
    "    X_train_val, # Input data\n",
    "    y_train_val, # Truth Output data\n",
    "    batch_size=1024, # Batch Size\n",
    "    epochs=30, # How many times will we iterate over the whole training dataset to train the model?\n",
    "    validation_split=0.25, # How much of the train dataset do we want to reserve as our validation split?\n",
    "    shuffle=True) #Do we want to to order of our training samples to be shuffled? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "-_6Hle_e6IXM",
    "outputId": "573db887-b46d-4641-ba6a-8ffe220d554a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Iteration #')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "iLFGZWL0--2n",
    "outputId": "f29291c5-6853-425a-de9d-2723878edc04"
   },
   "source": [
    "## Check model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import auc, roc_curve, accuracy_score\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    # plt.title(title)\n",
    "    cbar = plt.colorbar()\n",
    "    plt.clim(0, 1)\n",
    "    cbar.set_label(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "def plotRoc(fpr, tpr, auc, labels, linestyle, legend=True):\n",
    "    for _i, label in enumerate(labels):\n",
    "        plt.plot(\n",
    "            tpr[label],\n",
    "            fpr[label],\n",
    "            label='{} tagger, AUC = {:.1f}%'.format(label.replace('j_', ''), auc[label] * 100.0),\n",
    "            linestyle=linestyle,\n",
    "        )\n",
    "    plt.semilogy()\n",
    "    plt.xlabel(\"Signal Efficiency\")\n",
    "    plt.ylabel(\"Background Efficiency\")\n",
    "    plt.ylim(0.001, 1)\n",
    "    plt.grid(True)\n",
    "    if legend:\n",
    "        plt.legend(loc='upper left')\n",
    "    plt.figtext(0.25, 0.90, 'hls4ml', fontweight='bold', wrap=True, horizontalalignment='right', fontsize=14)\n",
    "\n",
    "\n",
    "def rocData(y, predict_test, labels):\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    auc1 = {}\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        df[label] = y[:, i]\n",
    "        df[label + '_pred'] = predict_test[:, i]\n",
    "\n",
    "        fpr[label], tpr[label], threshold = roc_curve(df[label], df[label + '_pred'])\n",
    "\n",
    "        auc1[label] = auc(fpr[label], tpr[label])\n",
    "    return fpr, tpr, auc1\n",
    "\n",
    "\n",
    "def makeRoc(y, predict_test, labels, linestyle='-', legend=True):\n",
    "    if 'j_index' in labels:\n",
    "        labels.remove('j_index')\n",
    "\n",
    "    fpr, tpr, auc1 = rocData(y, predict_test, labels)\n",
    "    plotRoc(fpr, tpr, auc1, labels, linestyle, legend=legend)\n",
    "    return predict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "1BF6-W_J-82M",
    "outputId": "a0631313-6fe4-402d-9ff5-14b172c38ded"
   },
   "outputs": [],
   "source": [
    "y_keras = model.predict(X_test)\n",
    "print(\"Accuracy: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_keras, axis=1))))\n",
    "plt.figure(figsize=(9, 9))\n",
    "_ = plotting.makeRoc(y_test, y_keras, le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribution, Sources, and Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was derived from two, seperate notebooks from the Tokyo Institute of Technology's ART.T458: Advanced Machine Learning course, and were originally authored by Prof. Naoaki Okazaki.\n",
    "\n",
    "The original notebooks (and accompanying material) can be found here: https://chokkan.github.io/deeplearning/\n",
    "\n",
    "Modifications to this notebook were done by Ben Hawks for the 2023 Fermilab and Brookhaven National Lab Summer Exchange School\n",
    "\n",
    "Various sources for images and other materials used is listed below: \n",
    "1. [Matrix Transpose Gif by Lucas Vieira via Wikipedia](https://commons.wikimedia.org/wiki/File:Matrix_transpose.gif)\n",
    "3. [Boolean Logic Gates Image/Symbols (Digilent) ](https://digilent.com/blog/logic-gates/)\n",
    "    * Original/Individual Symbols used (IEEE Std 91/91a-1991 \"Distinctive Shapes\") are originally via [Inductiveload via Wikipedia](https://en.wikipedia.org/wiki/Logic_gate#Symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "mlp_binary.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tiny-mlperf-env]",
   "language": "python",
   "name": "conda-env-.conda-tiny-mlperf-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
